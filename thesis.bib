@book{treiber-stack,
  title={Systems Programming: Coping with Parallelism},
  author={Thomas J. Watson IBM Research Center and Treiber, R.K.},
  series={Research Report RJ},
  url={https://books.google.nl/books?id=YQg3HAAACAAJ},
  year={1986},
  publisher={International Business Machines Incorporated, Thomas J. Watson Research Center}
}


@InProceedings{Owicki-Gries,
author="Wright, Daniel
and Batty, Mark
and Dongol, Brijesh",
editor="Huisman, Marieke
and P{\u{a}}s{\u{a}}reanu, Corina
and Zhan, Naijun",
title="Owicki-Gries Reasoning for C11 Programs with Relaxed Dependencies",
booktitle="Formal Methods",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="237--254",
abstract="Deductive verification techniques for C11 programs have advanced significantly in recent years with the development of operational semantics and associated logics for increasingly large fragments of C11. However, these semantics and logics have been developed in a restricted setting to avoid the thin-air-read problem. In this paper, we propose an operational semantics that leverages an intra-thread partial order (called semantic dependencies) induced by a recently developed denotational event-structure-based semantics. We prove that our operational semantics is sound and complete with respect to the denotational semantics. We present an associated logic that generalises a recent Owicki-Gries framework for RC11 (repaired C11), and demonstrate the use of this logic over several example proofs.",
isbn="978-3-030-90870-6"
}

@article{Kavanagh2018ADA,
  title={A denotational account of C11-style memory},
  author={Ryan Kavanagh and Stephen D. Brookes},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.04214},
  url={https://api.semanticscholar.org/CorpusID:4839024}
}

@inproceedings{wmm2016,
author = {Pichon-Pharabod, Jean and Sewell, Peter},
title = {A concurrency semantics for relaxed atomics that permits optimisation and avoids thin-air executions},
year = {2016},
isbn = {9781450335492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837614.2837616},
doi = {10.1145/2837614.2837616},
abstract = {Despite much research on concurrent programming languages, especially for Java and C/C++, we still do not have a satisfactory definition of their semantics, one that admits all common optimisations without also admitting undesired behaviour. Especially problematic are the ``thin-air'' examples involving high-performance concurrent accesses, such as C/C++11 relaxed atomics. The C/C++11 model is in a per-candidate-execution style, and previous work has identified a tension between that and the fact that compiler optimisations do not operate over single candidate executions in isolation; rather, they operate over syntactic representations that represent all executions. In this paper we propose a novel approach that circumvents this difficulty. We define a concurrency semantics for a core calculus, including relaxed-atomic and non-atomic accesses, and locks, that admits a wide range of optimisation while still forbidding the classic thin-air examples. It also addresses other problems relating to undefined behaviour. The basic idea is to use an event-structure representation of the current state of each thread, capturing all of its potential executions, and to permit interleaving of execution and transformation steps over that to reflect optimisation (possibly dynamic) of the code. These are combined with a non-multi-copy-atomic storage subsystem, to reflect common hardware behaviour. The semantics is defined in a mechanised and executable form, and designed to be implementable above current relaxed hardware and strong enough to support the programming idioms that C/C++11 does for this fragment. It offers a potential way forward for concurrent programming language semantics, beyond the current C/C++11 and Java models.},
booktitle = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {622–633},
numpages = {12},
keywords = {Relaxed memory models, Concurrency, C/C++},
location = {St. Petersburg, FL, USA},
series = {POPL '16}
}





@inproceedings{fuzz:Razzer,
  author    = {Jeong, Dae R. and Kim, Kyungtae and Shivakumar, Basavesh and Lee, Byoungyoung and Shin, Insik},
  booktitle = {2019 IEEE Symposium on Security and Privacy (SP)},
  title     = {Razzer: Finding Kernel Race Bugs through Fuzzing},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {754-768},
  keywords  = {Kernel;Instruction sets;Fuzzing;Tools;Computer bugs;Security;Static analysis;data-race;fuzzing;kernel;race-bugs},
  doi       = {10.1109/SP.2019.00017}
}



@inproceedings{fuzz:Krace,
  author    = {Xu, Meng and Kashyap, Sanidhya and Zhao, Hanqing and Kim, Taesoo},
  booktitle = {2020 IEEE Symposium on Security and Privacy (SP)},
  title     = {Krace: Data Race Fuzzing for Kernel File Systems},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {1643-1660},
  keywords  = {Kernel;Fuzzing;Instruction sets;Concurrent computing;Computer bugs;Synchronization;Delays},
  doi       = {10.1109/SP40000.2020.00078}
}


@inproceedings{fuzz:SlowFuzz,
  author    = {Petsios, Theofilos and Zhao, Jason and Keromytis, Angelos D. and Jana, Suman},
  title     = {SlowFuzz: Automated Domain-Independent Detection of Algorithmic Complexity Vulnerabilities},
  year      = {2017},
  isbn      = {9781450349468},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3133956.3134073},
  doi       = {10.1145/3133956.3134073},
  abstract  = {Algorithmic complexity vulnerabilities occur when the worst-case time/space complexity of an application is significantly higher than the respective average case for particular user-controlled inputs. When such conditions are met, an attacker can launch Denial-of-Service attacks against a vulnerable application by providing inputs that trigger the worst-case behavior. Such attacks have been known to have serious effects on production systems, take down entire websites, or lead to bypasses of Web Application Firewalls.Unfortunately, existing detection mechanisms for algorithmic complexity vulnerabilities are domain-specific and often require significant manual effort. In this paper, we design, implement, and evaluate SlowFuzz, a domain-independent framework for automatically finding algorithmic complexity vulnerabilities. SlowFuzz automatically finds inputs that trigger worst-case algorithmic behavior in the tested binary. SlowFuzz uses resource-usage-guided evolutionary search techniques to automatically find inputs that maximize computational resource utilization for a given application.We demonstrate that SlowFuzz successfully generates inputs that match the theoretical worst-case performance for several well-known algorithms. SlowFuzz was also able to generate a large number of inputs that trigger different algorithmic complexity vulnerabilities in real-world applications, including various zip parsers used in antivirus software, regular expression libraries used in Web Application Firewalls, as well as hash table implementations used in Web applications. In particular, SlowFuzz generated inputs that achieve 300-times slowdown in the decompression routine of the bzip utility, discovered regular expressions that exhibit matching times exponential in the input size, and also managed to automatically produce inputs that trigger a high number of collisions in PHP's default hashtable implementation.},
  booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2155–2168},
  numpages  = {14},
  keywords  = {algorithmic complexity attacks, dos attacks, fuzzing, resource exhaustion attacks},
  location  = {Dallas, Texas, USA},
  series    = {CCS '17}
}

@article{fuzz:Angora,
  author     = {Peng Chen and
                Hao Chen},
  title      = {Angora: Efficient Fuzzing by Principled Search},
  journal    = {CoRR},
  volume     = {abs/1803.01307},
  year       = {2018},
  url        = {http://arxiv.org/abs/1803.01307},
  eprinttype = {arXiv},
  eprint     = {1803.01307},
  timestamp  = {Wed, 08 Jun 2022 12:58:21 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1803-01307.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{fuzz:MOPT,
  author    = {Chenyang Lyu and Shouling Ji and Chao Zhang and Yuwei Li and Wei-Han Lee and Yu Song and Raheem Beyah},
  title     = {{MOPT}: Optimized Mutation Scheduling for Fuzzers},
  booktitle = {28th USENIX Security Symposium (USENIX Security 19)},
  year      = {2019},
  isbn      = {978-1-939133-06-9},
  address   = {Santa Clara, CA},
  pages     = {1949--1966},
  url       = {https://www.usenix.org/conference/usenixsecurity19/presentation/lyu},
  publisher = {USENIX Association},
  month     = aug
}

@inproceedings{fuzz:FairFuzz,
  author    = {Lemieux, Caroline and Sen, Koushik},
  title     = {FairFuzz: a targeted mutation strategy for increasing greybox fuzz testing coverage},
  year      = {2018},
  isbn      = {9781450359375},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3238147.3238176},
  doi       = {10.1145/3238147.3238176},
  abstract  = {In recent years, fuzz testing has proven itself to be one of the most effective techniques for finding correctness bugs and security vulnerabilities in practice. One particular fuzz testing tool, American Fuzzy Lop (AFL), has become popular thanks to its ease-of-use and bug-finding power. However, AFL remains limited in the bugs it can find since it simply does not cover large regions of code. If it does not cover parts of the code, it will not find bugs there. We propose a two-pronged approach to increase the coverage achieved by AFL. First, the approach automatically identifies branches exercised by few AFL-produced inputs (rare branches), which often guard code that is empirically hard to cover by naively mutating inputs. The second part of the approach is a novel mutation mask creation algorithm, which allows mutations to be biased towards producing inputs hitting a given rare branch. This mask is dynamically computed during fuzz testing and can be adapted to other testing targets. We implement this approach on top of AFL in a tool named FairFuzz. We conduct evaluation on real-world programs against state-of-the-art versions of AFL. We find that on these programs FairFuzz achieves high branch coverage at a faster rate that state-of-the-art versions of AFL. In addition, on programs with nested conditional structure, it achieves sustained increases in branch coverage after 24 hours (average 10.6\% increase). In qualitative analysis, we find that FairFuzz has an increased capacity to automatically discover keywords.},
  booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  pages     = {475–485},
  numpages  = {11},
  keywords  = {rare branches, fuzz testing, coverage-guided greybox fuzzing},
  location  = {Montpellier, France},
  series    = {ASE '18}
}

@misc{fuzz:aflfast,
  author = {Marcel, Böhme and Michal, Zalewski},
  title  = {AFLFast},
  year   = {2016},
  url    = {https://github.com/mboehme/aflfast}
}

@misc{mc:PAT,
  author = {LIU YANG},
  title  = {Model checking concurrent and real-time systems : the PAT approach},
  url    = {https://scholarbank.nus.edu.sg/handle/10635/17326}
}

@inproceedings{mc:civl,
  author    = {Kragl, Bernhard and Qadeer, Shaz},
  booktitle = {2021 Formal Methods in Computer Aided Design (FMCAD)},
  title     = {The Civl Verifier},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {143-152},
  keywords  = {Concurrent computing;Protocols;Atomic layer deposition;Programming;Tools;Linguistics;Cognition},
  doi       = {10.34727/2021/isbn.978-3-85448-046-4_23}
}


@inproceedings{mc:CBMC,
  author    = { Clarke, Edmund
               and Kroening, Daniel
               and Lerda, Flavio },
  title     = { A Tool for Checking {ANSI-C} Programs },
  booktitle = { Tools and Algorithms for the Construction and Analysis of Systems (TACAS 2004) },
  year      = { 2004 },
  publisher = { Springer },
  pages     = { 168--176 },
  isbn      = { 3-540-21299-X },
  series    = { Lecture Notes in Computer Science },
  volume    = { 2988 },
  editor    = { Kurt Jensen and Andreas Podelski }
}


@inproceedings{mc:PRISM,
  author    = {Kwiatkowska, Marta
               and Norman, Gethin
               and Parker, David},
  editor    = {Field, Tony
               and Harrison, Peter G.
               and Bradley, Jeremy
               and Harder, Uli},
  title     = {PRISM: Probabilistic Symbolic Model Checker},
  booktitle = {Computer Performance Evaluation: Modelling Techniques and Tools},
  year      = {2002},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {200--204},
  abstract  = {In this paper we describe PRISM, a tool being developed at the University of Birmingham for the analysis of probabilistic systems. PRISM supports three probabilistic models: discrete-time Markov chains, Markov decision processes and continuous-time Markov chains. Analysis is performed through model checking such systems against specifications written in the probabilistic temporal logics PCTL and CSL. The tool features three model checking engines: one symbolic, using BDDs (binary decision diagrams) and MTBDDs (multi-terminal BDDs); one based on sparse matrices; and one which combines both symbolic and sparse matrix methods. PRISM has been successfully used to analyse probabilistic termination, performance, and quality of service properties for a range of systems, including randomized distributed algorithms, manufacturing systems and workstation clusters.},
  isbn      = {978-3-540-46029-9}
}



@inproceedings{mc:NuSMV,
  author    = {A. Cimatti and E.M. Clarke and F. Giunchiglia and M. Roveri},
  title     = {NuSMV: a new Symbolic Model Verifier},
  booktitle = {Proceedings Eleventh Conference on Computer-Aided Verification {(CAV'99)}},
  pages     = {495-499},
  year      = 1999,
  editor    = {{N. H}albwachs and {D. P}eled},
  series    = {{L}ecture {N}otes in {C}omputer {S}cience},
  number    = {1633},
  address   = {{T}rento, {I}taly},
  month     = {July},
  publisher = {{S}pringer}
}

@phdthesis{mc:smv,
  author    = {McMillan, Kenneth Lauchlin},
  title     = {Symbolic model checking: an approach to the state explosion problem},
  year      = {1992},
  publisher = {Carnegie Mellon University},
  address   = {USA},
  abstract  = {Finite state models of concurrent systems grow exponentially as the number of components of the system increases. This is known widely as the state explosion problem in automatic verification, and has limited finite state verification methods to small systems. To avoid this problem, a method called symbolic model checking is proposed and studied. This method avoids building a state graph by using Boolean formulas to represent sets and relations. A variety of properties characterized by least and greatest fixed points can be verified purely by manipulations of these formulas using Ordered Binary Decision Diagrams. Theoretically, a structural class of sequential circuits is demonstrated whose transition relations can be represented by polynomial space OBDDs, though the number of states is exponential. This result is born out by experimental results on example circuits and systems. The most complex of these is the cache consistency protocol of a commercial distributed multiprocessor. The symbolic model checking technique revealed subtle errors in this protocol, resulting from complex execution sequences that would occur with very low probability in random simulation runs.In order to model the cache protocol, a language was developed for describing sequential circuits and protocols at various levels of abstraction. This language has a synchronous dataflow semantics, but allows nondeterminism and supports interleaving processes with shared variables. A system called SMV can automatically verify programs in this language with respect to temporal logic formulas, using the symbolic model checking technique.A technique for proving properties of inductively generated classes of finite state systems is also developed. The proof is checked automatically, but requires a user supplied process called a process invariant to act as an inductive hypothesis. An invariant is developed for the distributed cache protocol, allowing properties of systems with an arbitrary number of processors to be proved.Finally, an alternative method is developed for avoiding the state explosion in the case of asynchronous control circuits. This technique is based on the unfolding of Petri nets, and is used to check for hazards in a distributed mutual exclusion circuit.},
  note      = {UMI Order No. GAX92-24209}
}

@article{mc:spin,
  author   = {Holzmann, G.J.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {The model checker SPIN},
  year     = {1997},
  volume   = {23},
  number   = {5},
  pages    = {279-295},
  keywords = {Software systems;Application software;Distributed algorithms;Control system synthesis;Algorithm design and analysis;Error correction codes;Telephony;Design methodology;Concurrent computing;Message passing},
  doi      = {10.1109/32.588521}
}



@article{mc:Clarke,
  author     = {Clarke, E. M. and Emerson, E. A. and Sistla, A. P.},
  title      = {Automatic verification of finite-state concurrent systems using temporal logic specifications},
  year       = {1986},
  issue_date = {April 1986},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {8},
  number     = {2},
  issn       = {0164-0925},
  url        = {https://doi.org/10.1145/5397.5399},
  doi        = {10.1145/5397.5399},
  abstract   = {We give an efficient procedure for verifying that a finite-state concurrent system meets a specification expressed in a (propositional, branching-time) temporal logic. Our algorithm has complexity linear in both the size of the specification and the size of the global state graph for the concurrent system. We also show how this approach can be adapted to handle fairness. We argue that our technique can provide a practical alternative to manual proof construction or use of a mechanical theorem prover for verifying many finite-state concurrent systems. Experimental results show that state machines with several hundred states can be checked in a matter of seconds.},
  journal    = {ACM Trans. Program. Lang. Syst.},
  month      = {apr},
  pages      = {244–263},
  numpages   = {20}
}





@inproceedings{CollAFL,
  author    = {Gan, Shuitao and Zhang, Chao and Qin, Xiaojun and Tu, Xuwen and Li, Kang and Pei, Zhongyu and Chen, Zuoning},
  booktitle = {2018 IEEE Symposium on Security and Privacy (SP)},
  title     = {CollAFL: Path Sensitive Fuzzing},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {679-696},
  keywords  = {Fuzzing;Security;Instruments;Computer bugs;Grammar;Target tracking;fuzzing;vulnerability discovery},
  doi       = {10.1109/SP.2018.00040}
}

@misc{TriforceAFL,
  author = {Nccgroup},
  title  = {AFL/QEMU fuzzing with full-system emulation.},
  url    = {https://github.com/nccgroup/TriforceAFL}
}

@misc{kAFL,
  author = {Intel Labs},
  title  = {HW-assisted Feedback Fuzzer for x86 VMs},
  url    = {https://github.com/IntelLabs/kAFL}
}

@misc{Driller,
  author = {Shellphish},
  title  = {Driller: augmenting AFL with symbolic execution!},
  url    = {https://github.com/shellphish/driller}
}

@inproceedings{sage,
  author    = {Godefroid, Patrice},
  title     = {Random testing for security: blackbox vs. whitebox fuzzing},
  year      = {2007},
  isbn      = {9781595938817},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1292414.1292416},
  doi       = {10.1145/1292414.1292416},
  abstract  = {Fuzz testing is an effective technique for finding security vulnerabilities in software. Fuzz testing is a form of blackbox random testing which randomly mutates well-formed inputs and tests the program on the resulting data. In some cases, grammars are used to randomly generate the well-formed inputs. This also allows the tester to encode application-specific knowledge (such as corner cases of particular interest) as part of the grammar, and to specify test heuristics by assigning probabilistic weights to production rules. Although fuzz testing can be remarkably effective, the limitations of blackbox random testing are well-known. For instance, the then branch of the conditional statement "if (x==10) then" has only one in 232 chances of being exercised if x is a randomly chosen 32-bit input value. This intuitively explains why random testing usually provides low code coverage.Recently, we have proposed an alternative approach of whitebox fuzz testing [4], building upon recent advances in dynamic symbolic execution and test generation [2]. Starting with a well-formed input, our approach symbolically executes the program dynamically and gathers constraints on inputs from conditional statements encountered along the way. The collected constraints are then systematically negated and solved with a constraint solver, yielding new inputs that exercise different execution paths in the program. This process is repeated using a novel search algorithm with a coverage-maximizing heuristic designed to find defects as fast as possible in large search spaces. For example, symbolic execution of the above code fragment on the input x = 0 generates the constraint x ≠ 10. Once this constraint is negated and solved, it yields x = 10, which gives us a new input that causes the program to follow the then branch of the given conditional statement.We have implemented this approach in SAGE (Scalable, Automated, Guided Execution), a tool based on x86 instruction-level tracing and emulation for whitebox fuzzing of file-reading Windows applications. While still in an early stage of development and deployment, SAGE has already discovered more than 30 new bugs in large shipped Windows applications including image processors, media players and file decoders. Several of these bugs are potentially exploitable memory access violations.In this talk, I will briefly review blackbox fuzzing for security testing. Then, I will present an overview of our recent work on whitebox fuzzing [4] (joint work with Michael Y. Levin and David Molnar), with an emphasis on the key algorithms and techniques needed to make this approach effective and scalable (see also [1, 3]).},
  booktitle = {Proceedings of the 2nd International Workshop on Random Testing: Co-Located with the 22nd IEEE/ACM International Conference on Automated Software Engineering (ASE 2007)},
  pages     = {1},
  numpages  = {1},
  keywords  = {software testing, security, program verification, automatic test generation},
  location  = {Atlanta, Georgia},
  series    = {RT '07}
}








@article{millerFuzzer,
  title     = {An empirical study of the reliability of UNIX utilities},
  author    = {Miller, Barton P and Fredriksen, Lars and So, Bryan},
  journal   = {Communications of the ACM},
  volume    = {33},
  number    = {12},
  pages     = {32--44},
  year      = {1990},
  publisher = {ACM New York, NY, USA}
}



@article{Kater,
  author     = {Kokologiannakis, Michalis and Lahav, Ori and Vafeiadis, Viktor},
  title      = {Kater: Automating Weak Memory Model Metatheory and Consistency Checking},
  year       = {2023},
  issue_date = {January 2023},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {7},
  number     = {POPL},
  url        = {https://doi.org/10.1145/3571212},
  doi        = {10.1145/3571212},
  abstract   = {The metatheory of axiomatic weak memory models covers questions like the 
                correctness of compilation mappings from one model to another and the 
                correctness of local program transformations according to a given model---topics 
                usually requiring lengthy human investigation. 
                We show that these questions can be solved by answering a 
                more basic question: "Given two memory models, is one weaker than the other?" 
                Moreover, for a wide class of axiomatic memory models, we show that this 
                basic question can be reduced to a language inclusion problem between regular 
                languages, which is decidable. 
                
                Similarly, implementing an efficient check for whether an execution graph is 
                consistent according to a given memory model has required non-trivial manual 
                effort. Again, we show that such efficient checks can be derived automatically 
                for a wide class of axiomatic memory models, and that incremental consistency checks 
                can be incorporated in GenMC, a state-of-the-art model checker for concurrent 
                programs. As a result, we get the first time- and space-efficient bounded 
                verifier taking the axiomatic memory model as an input parameter.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {jan},
  articleno  = {19},
  numpages   = {29},
  keywords   = {Kleene Algebra with Tests, Model Checking, Weak Memory Models}
}

@misc{LKMM,
  author = {Paul, McKenney and Ulrich, Weigand and Andrea, Parri and  Boqun, Feng and Alan, Stern},
  title  = {Linux-Kernel Memory Model},
  year   = {2020},
  url    = {https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p0124r7.html}
}

@article{IMM,
  author     = {Podkopaev, Anton and Lahav, Ori and Vafeiadis, Viktor},
  title      = {Bridging the gap between programming languages and hardware weak memory models},
  year       = {2019},
  issue_date = {January 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {3},
  number     = {POPL},
  url        = {https://doi.org/10.1145/3290382},
  doi        = {10.1145/3290382},
  abstract   = {We develop a new intermediate weak memory model, IMM, as a way of modularizing the proofs of correctness of compilation from concurrent programming languages with weak memory consistency semantics to mainstream multi-core architectures, such as POWER and ARM. We use IMM to prove the correctness of compilation from the promising semantics of Kang et al. to POWER (thereby correcting and improving their result) and ARMv7, as well as to the recently revised ARMv8 model. Our results are mechanized in Coq, and to the best of our knowledge, these are the first machine-verified compilation correctness results for models that are weaker than x86-TSO.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {jan},
  articleno  = {69},
  numpages   = {31},
  keywords   = {C11 memory model, IMM, Weak memory consistency, promising semantics}
}

@article{RC11,
  author     = {Lahav, Ori and Vafeiadis, Viktor and Kang, Jeehoon and Hur, Chung-Kil and Dreyer, Derek},
  title      = {Repairing sequential consistency in C/C++11},
  year       = {2017},
  issue_date = {June 2017},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {52},
  number     = {6},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/3140587.3062352},
  doi        = {10.1145/3140587.3062352},
  abstract   = {The C/C++11 memory model defines the semantics of concurrent memory accesses in C/C++, and in particular supports racy "atomic" accesses at a range of different consistency levels, from very weak consistency ("relaxed") to strong, sequential consistency ("SC"). Unfortunately, as we observe in this paper, the semantics of SC atomic accesses in C/C++11, as well as in all proposed strengthenings of the semantics, is flawed, in that (contrary to previously published results) both suggested compilation schemes to the Power architecture are unsound. We propose a model, called RC11 (for Repaired C11), with a better semantics for SC accesses that restores the soundness of the compilation schemes to Power, maintains the DRF-SC guarantee, and provides stronger, more useful, guarantees to SC fences. In addition, we formally prove, for the first time, the correctness of the proposed stronger compilation schemes to Power that preserve load-to-store ordering and avoid "out-of-thin-air" reads.},
  journal    = {SIGPLAN Not.},
  month      = {jun},
  pages      = {618–632},
  numpages   = {15},
  keywords   = {sequential consistency, declarative semantics, Weak memory models, C++11}
}










@article{CC90,
  author    = {E. J. Chikofsky and J. H. Cross},
  title     = {Reverse Engineering and Design Recovery: A Taxonomy},
  journal   = {IEEE Software},
  year      = 1990,
  volume    = 7,
  number    = 1,
  pages     = {13--17},
  publisher = {IEEE Computer Society Press}
}

@inproceedings{left-right,
  title        = {{Brief Announcement: Left-Right - A Concurrency Control Technique with Wait-Free Population Oblivious Reads}},
  author       = {Ramalhete, Pedro and Correia, Andreia},
  url          = {https://hal.science/hal-01207881},
  booktitle    = {{DISC 2015}},
  address      = {Tokyo, Japan},
  organization = {{Toshimitsu Masuzawa and Koichi Wada}},
  editor       = {Yoram Moses and Matthieu Roy},
  publisher    = {{Springer-Verlag Berlin Heidelberg}},
  series       = {29th International Symposium on Distributed Computing},
  volume       = {LNCS 9363},
  year         = {2015},
  month        = Oct,
  pdf          = {https://hal.science/hal-01207881/file/39-BA.pdf},
  hal_id       = {hal-01207881},
  hal_version  = {v1}
}

@misc{lockfree-xenium,
  author = {Manuel Pöter},
  title  = {xenium},
  url    = {https://github.com/mpoeter/xenium}
}

@misc{lockfree-DNedic,
  author = {Djordje Nedic},
  title  = {lockfree},
  url    = {https://github.com/DNedic/lockfree}
}

@misc{dekker-fence-impl,
  author = {Anthony Williams},
  title  = {Implementing Dekker's algorithm with Fences},
  url    = {https://www.justsoftwaresolutions.co.uk/threading/implementing_dekkers_algorithm_with_fences.html}
}

@article{mcs-lock,
  author     = {Mellor-Crummey, John M. and Scott, Michael L.},
  title      = {Synchronization without contention},
  year       = {1991},
  issue_date = {Apr. 1991},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {19},
  number     = {2},
  issn       = {0163-5964},
  url        = {https://doi.org/10.1145/106975.106999},
  doi        = {10.1145/106975.106999},
  journal    = {SIGARCH Comput. Archit. News},
  month      = {apr},
  pages      = {269–278},
  numpages   = {10}
}
@misc{mcs-lock-impl,
  author = {cbloom},
  title  = {MCS list-based lock},
  url    = {https://cbloomrants.blogspot.com/2011/07/07-18-11-mcs-list-based-lock_18.html}
}

@misc{mpmc-queue-impl,
  author = {cbloom},
  title  = {A look at some bounded queues - part 2},
  url    = {https://cbloomrants.blogspot.com/2011/07/07-30-11-look-at-some-bounded-queues.html}
}

@article{chase-lev-deque-impl,
  author     = {L\^{e}, Nhat Minh and Pop, Antoniu and Cohen, Albert and Zappa Nardelli, Francesco},
  title      = {Correct and efficient work-stealing for weak memory models},
  year       = {2013},
  issue_date = {August 2013},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {48},
  number     = {8},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/2517327.2442524},
  doi        = {10.1145/2517327.2442524},
  abstract   = {Chase and Lev's concurrent deque is a key data structure in shared-memory parallel programming and plays an essential role in work-stealing schedulers. We provide the first correctness proof of an optimized implementation of Chase and Lev's deque on top of the POWER and ARM architectures: these provide very relaxed memory models, which we exploit to improve performance but considerably complicate the reasoning. We also study an optimized x86 and a portable C11 implementation, conducting systematic experiments to evaluate the impact of memory barrier optimizations. Our results demonstrate the benefits of hand tuning the deque code when running on top of relaxed memory models.},
  journal    = {SIGPLAN Not.},
  month      = {feb},
  pages      = {69-80},
  numpages   = {12},
  keywords   = {work-stealing, relaxed memory model, proof, lock-free algorithm}
}



@book{Whe95,
  author    = {Colin Wheildon},
  title     = {Type \& Layout},
  publisher = {Strathmore Press},
  note      = {(ISBN 0 9624891 5 8)},
  year      = 1995
}

@inproceedings{c11tester,
  author    = {Luo, Weiyu and Demsky, Brian},
  title     = {C11Tester: a race detector for C/C++ atomics},
  year      = {2021},
  isbn      = {9781450383172},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3445814.3446711},
  doi       = {10.1145/3445814.3446711},
  abstract  = {Writing correct concurrent code that uses atomics under the C/C++ memory model is extremely difficult. We present C11Tester, a race detector for the C/C++ memory model that can explore executions in a larger fragment of the C/C++ memory model than previous race detector tools. Relative to previous work, C11Tester's larger fragment includes behaviors that are exhibited by ARM processors. C11Tester uses a new constraint-based algorithm to implement modification order that is optimized to allow C11Tester to make decisions in terms of application-visible behaviors. We evaluate C11Tester on several benchmark applications, and compare C11Tester's performance to both tsan11rec, the state of the art tool that controls scheduling for C/C++; and tsan11, the state of the art tool that does not control scheduling.},
  booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {630-646},
  numpages  = {17},
  keywords  = {memory models, data races, concurrency, C++11},
  location  = {Virtual, USA},
  series    = {ASPLOS '21}
}

@article{cdschecker,
  author     = {Norris, Brian and Demsky, Brian},
  title      = {CDSchecker: checking concurrent data structures written with C/C++ atomics},
  year       = {2013},
  issue_date = {October 2013},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {48},
  number     = {10},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/2544173.2509514},
  doi        = {10.1145/2544173.2509514},
  abstract   = {Writing low-level concurrent software has traditionally required intimate knowledge of the entire toolchain and often has involved coding in assembly. New language standards have extended C and C++ with support for low-level atomic operations and a weak memory model, enabling developers to write portable and efficient multithreaded code.Developing correct low-level concurrent code is well-known to be especially difficult under a weak memory model, where code behavior can be surprising. Building reliable concurrent software using C/C++ low-level atomic operations will likely require tools that help developers discover unexpected program behaviors.In this paper we present CDSChecker, a tool for exhaustively exploring the behaviors of concurrent code under the C/C++ memory model. We develop several novel techniques for modeling the relaxed behaviors allowed by the memory model and for minimizing the number of execution behaviors that CDSChecker must explore. We have used CDSChecker to exhaustively unit test several concurrent data structure implementations on specific input and have discovered errors in both a recently published C11 implementation of a work-stealing queue and a single producer, single consumer queue implementation.},
  journal    = {SIGPLAN Not.},
  month      = {oct},
  pages      = {131-150},
  numpages   = {20},
  keywords   = {relaxed memory model, model checking}
}

@inproceedings{genmc,
  author    = {Kokologiannakis, Michalis
               and Vafeiadis, Viktor},
  editor    = {Silva, Alexandra
               and Leino, K. Rustan M.},
  title     = {GenMC: A Model Checker for Weak Memory Models},
  booktitle = {Computer Aided Verification},
  year      = {2021},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {427--440},
  abstract  = {GenMC is an LLVM-based state-of-the-art stateless model checker for concurrent C/C++ programs. Its modular infrastructure allows it to support complex memory models, such as RC11 and IMM, and makes it easy to extend to support further axiomatic memory models.},
  isbn      = {978-3-030-81685-8}
}

@misc{afl,
  author = {Michal Zalewski},
  title  = {American Fuzzy Lop},
  year   = {2014},
  url    = {https://lcamtuf.coredump.cx/afl/}
}

@inproceedings{muzz,
  author    = {Hongxu Chen and Shengjian Guo and Yinxing Xue and Yulei Sui and Cen Zhang and Yuekang Li and Haijun Wang and Yang Liu},
  title     = {{MUZZ}: Thread-aware Grey-box Fuzzing for Effective Bug Hunting in Multithreaded Programs},
  booktitle = {29th USENIX Security Symposium (USENIX Security 20)},
  year      = {2020},
  isbn      = {978-1-939133-17-5},
  pages     = {2325--2342},
  url       = {https://www.usenix.org/conference/usenixsecurity20/presentation/chen-hongxu},
  publisher = {USENIX Association},
  month     = aug
}

@inproceedings{conzzer,
  author = {Jiang, Zu-Ming and Bai, Jia-Ju and Lu, Kangjie and Hu, Shi-Min},
  year   = {2022},
  month  = {01},
  pages  = {},
  title  = {Context-Sensitive and Directional Concurrency Fuzzing for Data-Race Detection},
  doi    = {10.14722/ndss.2022.24296}
}

@inproceedings{10.1145/1736020.1736040,
  author    = {Burckhardt, Sebastian and Kothari, Pravesh and Musuvathi, Madanlal and Nagarakatte, Santosh},
  title     = {A randomized scheduler with probabilistic guarantees of finding bugs},
  year      = {2010},
  isbn      = {9781605588391},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1736020.1736040},
  doi       = {10.1145/1736020.1736040},
  abstract  = {This paper presents a randomized scheduler for finding concurrency bugs. Like current stress-testing methods, it repeatedly runs a given test program with supplied inputs. However, it improves on stress-testing by finding buggy schedules more effectively and by quantifying the probability of missing concurrency bugs. Key to its design is the characterization of the depth of a concurrency bug as the minimum number of scheduling constraints required to find it. In a single run of a program with n threads and k steps, our scheduler detects a concurrency bug of depth d with probability at least 1/nkd-1. We hypothesize that in practice, many concurrency bugs (including well-known types such as ordering errors, atomicity violations, and deadlocks) have small bug-depths, and we confirm the efficiency of our schedule randomization by detecting previously unknown and known concurrency bugs in several production-scale concurrent programs.},
  booktitle = {Proceedings of the Fifteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {167-178},
  numpages  = {12},
  keywords  = {concurrency, race conditions, randomized algorithms, testing},
  location  = {Pittsburgh, Pennsylvania, USA},
  series    = {ASPLOS XV}
}

@article{10.1145/1735970.1736040,
  author     = {Burckhardt, Sebastian and Kothari, Pravesh and Musuvathi, Madanlal and Nagarakatte, Santosh},
  title      = {A randomized scheduler with probabilistic guarantees of finding bugs},
  year       = {2010},
  issue_date = {March 2010},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {38},
  number     = {1},
  issn       = {0163-5964},
  url        = {https://doi.org/10.1145/1735970.1736040},
  doi        = {10.1145/1735970.1736040},
  abstract   = {This paper presents a randomized scheduler for finding concurrency bugs. Like current stress-testing methods, it repeatedly runs a given test program with supplied inputs. However, it improves on stress-testing by finding buggy schedules more effectively and by quantifying the probability of missing concurrency bugs. Key to its design is the characterization of the depth of a concurrency bug as the minimum number of scheduling constraints required to find it. In a single run of a program with n threads and k steps, our scheduler detects a concurrency bug of depth d with probability at least 1/nkd-1. We hypothesize that in practice, many concurrency bugs (including well-known types such as ordering errors, atomicity violations, and deadlocks) have small bug-depths, and we confirm the efficiency of our schedule randomization by detecting previously unknown and known concurrency bugs in several production-scale concurrent programs.},
  journal    = {SIGARCH Comput. Archit. News},
  month      = {mar},
  pages      = {167-178},
  numpages   = {12},
  keywords   = {concurrency, race conditions, randomized algorithms, testing}
}

@article{pct,
  author     = {Burckhardt, Sebastian and Kothari, Pravesh and Musuvathi, Madanlal and Nagarakatte, Santosh},
  title      = {A randomized scheduler with probabilistic guarantees of finding bugs},
  year       = {2010},
  issue_date = {March 2010},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {45},
  number     = {3},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/1735971.1736040},
  doi        = {10.1145/1735971.1736040},
  abstract   = {This paper presents a randomized scheduler for finding concurrency bugs. Like current stress-testing methods, it repeatedly runs a given test program with supplied inputs. However, it improves on stress-testing by finding buggy schedules more effectively and by quantifying the probability of missing concurrency bugs. Key to its design is the characterization of the depth of a concurrency bug as the minimum number of scheduling constraints required to find it. In a single run of a program with n threads and k steps, our scheduler detects a concurrency bug of depth d with probability at least 1/nkd-1. We hypothesize that in practice, many concurrency bugs (including well-known types such as ordering errors, atomicity violations, and deadlocks) have small bug-depths, and we confirm the efficiency of our schedule randomization by detecting previously unknown and known concurrency bugs in several production-scale concurrent programs.},
  journal    = {SIGPLAN Not.},
  month      = {mar},
  pages      = {167-178},
  numpages   = {12},
  keywords   = {concurrency, race conditions, randomized algorithms, testing}
}

@inproceedings{pctwm,
  author    = {Gao, Mingyu and Chakraborty, Soham and Ozkan, Burcu Kulahcioglu},
  title     = {Probabilistic Concurrency Testing for Weak Memory Programs},
  year      = {2023},
  isbn      = {9781450399166},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3575693.3575729},
  doi       = {10.1145/3575693.3575729},
  abstract  = {The Probabilistic Concurrency Testing (PCT) algorithm that provides theoretical guarantees on the probability of detecting concurrency bugs does not apply to weak memory programs. The PCT algorithm builds on the interleaving semantics of sequential consistency, which does not hold for weak memory concurrency. It is because weak memory concurrency allows additional behaviors that cannot be produced by any interleaving execution.  
               
               In this paper, we generalize PCT to address weak memory concurrency and present Probabilistic Concurrency Testing for Weak Memory (PCTWM). We empirically evaluate PCTWM on a set of well-known weak memory program benchmarks in comparison to the state-of-the-art weak memory testing tool C11Tester. Our results show that PCTWM can detect concurrency bugs more frequently than C11Tester.},
  booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages     = {603-616},
  numpages  = {14},
  keywords  = {Concurrency, Randomized algorithms, Testing, Weak memory},
  location  = {Vancouver, BC, Canada},
  series    = {ASPLOS 2023}
}

@inproceedings{rff,
  author    = {Meng, Ruijie and P\^{\i}rlea, George and Roychoudhury, Abhik and Sergey, Ilya},
  title     = {Greybox Fuzzing of Distributed Systems},
  year      = {2023},
  isbn      = {9798400700507},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3576915.3623097},
  doi       = {10.1145/3576915.3623097},
  abstract  = {Grey-box fuzzing is the lightweight approach of choice for finding bugs in sequential programs. It provides a balance between efficiency and effectiveness by conducting a biased random search over the domain of program inputs using a feedback function from observed test executions. For distributed system testing, however, the state-of-practice is represented today by only black-box tools that do not attempt to infer and exploit any knowledge of the system's past behaviours to guide the search for bugs.In this work, we present MALLORY: the first framework for grey-box fuzz-testing of distributed systems. Unlike popular black-box distributed system fuzzers, such as JEPSEN, that search for bugs by randomly injecting network partitions and node faults or by following human-defined schedules, MALLORY is adaptive. It exercises a novel metric to learn how to maximize the number of observed system behaviors by choosing different sequences of faults, thus increasing the likelihood of finding new bugs. Our approach relies on timeline-driven testing. MALLORY dynamically constructs Lamport timelines of the system behaviour and further abstracts these timelines into happens-before summaries, which serve as a feedback function guiding the fuzz campaign. Subsequently, MALLORY reactively learns a policy using Q-learning, enabling it to introduce faults guided by its real-time observation of the summaries.We have evaluated MALLORY on a diverse set of widely-used industrial distributed systems. Compared to the start-of-the-art black-box fuzzer JEPSEN, MALLORY explores 54.27\% more distinct states within 24 hours while achieving a speed-up of 2.24X. At the same time, MALLORY finds bugs 1.87X faster, thereby finding more bugs within the given time budget. MALLORY discovered 22 zero-day bugs (of which 18 were confirmed by developers), including 10 new vulnerabilities, in rigorously tested distributed systems such as Braft, Dqlite and Redis. 6 new CVEs have been assigned.},
  booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {1615-1629},
  numpages  = {15},
  keywords  = {distributed systems, greybox fuzzing, reactive system testing},
  location  = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
  series    = {CCS '23}
}

@inproceedings{schfuzz,
  title     = {Schfuzz: Detecting Concurrency Bugs with Feedback-Guided Fuzzing},
  author    = {Hiromasa Ito and Yutaka Matsubara and Hiroaki Takada},
  booktitle = {International Conference on Evaluation of Novel Approaches to Software Engineering},
  year      = {2023},
  url       = {https://api.semanticscholar.org/CorpusID:258362077}
}

@article{Weakestmo2,
  author     = {Moiseenko, Evgenii and Kokologiannakis, Michalis and Vafeiadis, Viktor},
  title      = {Model checking for a multi-execution memory model},
  year       = {2022},
  issue_date = {October 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {6},
  number     = {OOPSLA2},
  url        = {https://doi.org/10.1145/3563315},
  doi        = {10.1145/3563315},
  abstract   = {Multi-execution memory models, such as Promising and Weakestmo, are an advanced class of weak memory  
                consistency models that justify certain outcomes of a concurrent program by considering multiple candidate executions collectively. While this key characteristic allows them to support effective compilation to hardware models and a wide range of compiler optimizations, it makes reasoning about them substantially more difficult. In particular, we observe that Promising and Weakestmo inhibit effective model checking because they allow some suprisingly weak behaviors that cannot be generated by examining one execution at a time.  
                
                We therefore introduce Weakestmo2, a strengthening of Weakestmo by constraining its multi-execution  
                nature, while preserving the important properties of Weakestmo: DRF theorems, compilation to hardware models, and correctness of local program transformations. Our strengthening rules out a class of surprisingly weak program behaviors, which we attempt to characterize with the help of two novel properties: load buffering race freedom and certification locality. In addition, we develop WMC, a model checker for Weakestmo2 with performance close to that of the best tools for per-execution models.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {oct},
  articleno  = {152},
  numpages   = {28},
  keywords   = {Weak memory models, model checking}
}

@inproceedings{ConFuzz,
  author    = {Vinesh, Nischai
               and Sethumadhavan, M.},
  editor    = {Luhach, Ashish Kumar
               and Kosa, Janos Arpad
               and Poonia, Ramesh Chandra
               and Gao, Xiao-Zhi
               and Singh, Dharm},
  title     = {ConFuzz---A Concurrency Fuzzer},
  booktitle = {First International Conference on Sustainable Technologies for Computational Intelligence},
  year      = {2020},
  publisher = {Springer Singapore},
  address   = {Singapore},
  pages     = {667--691},
  abstract  = {Concurrency bugs are as equally vulnerable as the bugs found in the single-threaded programs and these bugs can be exploited using concurrency attacks. Unfortunately, there is not much literature available in detecting various kinds of concurrency issues in a multi-threaded program due to its complexity and uncertainty. In this paper, we aim at detecting concurrency bugs by using directed evolutionary fuzzing with the help of static analysis of the source code. Concurrency bug detection involves two main entities: an input and a particular thread execution order. The evolutionary part of fuzzing will prefer inputs that involve memory access patterns across threads (data flow interleaving) and thread ordering that disturb the data dependence more and direct them to trigger concurrency bugs. This paper suggests the idea of a concurrency fuzzer, which is first of its kind. We use a combination of LLVM, Thread Sanitizer and fuzzing techniques to detect various concurrency issues in an application. The source code of the application is statically analyzed for various paths, from the different thread related function calls to the main function. Every basic block in these paths are assigned a unique ID and a weight based on the distance of the basic block from the thread function calls. These basic blocks are instrumented to print their ID and weight upon execution. The knowledge about the basic blocks in the sliced paths are used to generate new sets of inputs from the old ones, thus covering even more basic blocks in the path and thereby increasing the chances of hitting a concurrency warning. We use Thread Sanitizer present in the LLVM compiler infrastructure to detect the concurrency bug warnings while executing each input. The inputs are directed to discover even new address locations with possible concurrency issues. The system was tested on three simple multi-threaded applications pigz, pbzip2, and pixz. The results show a quicker detection of unique addresses in the application with possible concurrency issues.},
  isbn      = {978-981-15-0029-9}
}

@article{AutoInter,
  author     = {Ko, Youngjoo and Zhu, Bin and Kim, Jong},
  title      = {Fuzzing with automatically controlled interleavings to detect concurrency bugs},
  year       = {2022},
  issue_date = {Sep 2022},
  publisher  = {Elsevier Science Inc.},
  address    = {USA},
  volume     = {191},
  number     = {C},
  issn       = {0164-1212},
  url        = {https://doi.org/10.1016/j.jss.2022.111379},
  doi        = {10.1016/j.jss.2022.111379},
  journal    = {J. Syst. Softw.},
  month      = {sep},
  numpages   = {15},
  keywords   = {Reliability, Multi-threading, Concurrency vulnerabilities, Bug detection, Fuzzing}
}

@inproceedings{heuristic,
  author    = {Liu, Changming and Zou, Deqing and Luo, Peng and Zhu, Bin B. and Jin, Hai},
  title     = {A Heuristic Framework to Detect Concurrency Vulnerabilities},
  year      = {2018},
  isbn      = {9781450365697},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3274694.3274718},
  doi       = {10.1145/3274694.3274718},
  abstract  = {With a growing demand of concurrent software to exploit multi-core hardware capability, concurrency vulnerabilities have become an inevitable threat to the security of today's IT industry. Existing concurrent program detection schemes focus mainly on detecting concurrency errors such as data races, atomicity violation, etc., with little attention paid to detect concurrency vulnerabilities that may be exploited to infringe security. In this paper, we propose a heuristic framework that combines both static analysis and fuzz testing to detect targeted concurrency vulnerabilities such as concurrency buffer overflow, double free, and use-after-free. The static analysis locates sensitive concurrent operations in a concurrent program, categorizes each finding into a potential type of concurrency vulnerability, and determines the execution order of the sensitive operations in each finding that would trigger the suspected concurrency vulnerability. The results are then plugged into the fuzzer with the execution order fixed by the static analysis in order to trigger the suspected concurrency vulnerabilities.In order to introduce more variance which increases possibility that the concurrency errors can be triggered, we also propose manipulation of thread scheduling priority to enable a fuzzer such as AFL to effectively explore thread interleavings in testing a concurrent program. To the best of our knowledge, this is the first fuzzer that is capable of effectively exploring concurrency errors.In evaluating the proposed heuristic framework with a benchmark suit of six real-world concurrent C programs, the framework detected two concurrency vulnerabilities for the proposed concurrency vulnerability detection, both being confirmed to be true positives, and produced three new crashes for the proposed interleaving exploring fuzzer that existing fuzzers could not produce. These results demonstrate the power and effectiveness of the proposed heuristic framework in detecting concurrency errors and vulnerabilities.},
  booktitle = {Proceedings of the 34th Annual Computer Security Applications Conference},
  pages     = {529-541},
  numpages  = {13},
  keywords  = {Concurrency Vulnerabilities, Fuzzing Test, Thread Schedule},
  location  = {San Juan, PR, USA},
  series    = {ACSAC '18}
}

@inproceedings{sel4,
  author    = {Klein, Gerwin and Elphinstone, Kevin and Heiser, Gernot and Andronick, June and Cock, David and Derrin, Philip and Elkaduwe, Dhammika and Engelhardt, Kai and Kolanski, Rafal and Norrish, Michael and Sewell, Thomas and Tuch, Harvey and Winwood, Simon},
  title     = {seL4: formal verification of an OS kernel},
  year      = {2009},
  isbn      = {9781605587523},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1629575.1629596},
  doi       = {10.1145/1629575.1629596},
  abstract  = {Complete formal verification is the only known way to guarantee that a system is free of programming errors.We present our experience in performing the formal, machine-checked verification of the seL4 microkernel from an abstract specification down to its C implementation. We assume correctness of compiler, assembly code, and hardware, and we used a unique design approach that fuses formal and operating systems techniques. To our knowledge, this is the first formal proof of functional correctness of a complete, general-purpose operating-system kernel. Functional correctness means here that the implementation always strictly follows our high-level abstract specification of kernel behaviour. This encompasses traditional design and implementation safety properties such as the kernel will never crash, and it will never perform an unsafe operation. It also proves much more: we can predict precisely how the kernel will behave in every possible situation.seL4, a third-generation microkernel of L4 provenance, comprises 8,700 lines of C code and 600 lines of assembler. Its performance is comparable to other high-performance L4 kernels.},
  booktitle = {Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles},
  pages     = {207-220},
  numpages  = {14},
  keywords  = {isabelle/hol, l4, microkernel, sel4},
  location  = {Big Sky, Montana, USA},
  series    = {SOSP '09}
}

@inproceedings{infer,
  author    = {Calcagno, Cristiano
               and Distefano, Dino},
  editor    = {Bobaru, Mihaela
               and Havelund, Klaus
               and Holzmann, Gerard J.
               and Joshi, Rajeev},
  title     = {Infer: An Automatic Program Verifier for Memory Safety of C Programs},
  booktitle = {NASA Formal Methods},
  year      = {2011},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {459--465},
  abstract  = {Infer is a new automatic program verification tool aimed at proving memory safety of C programs. It attempts to build a compositional proof of the program at hand by composing proofs of its constituent modules (functions/procedures). Bugs are extracted from failures of proof attempts. We describe the main features of Infer and some of the main ideas behind it.},
  isbn      = {978-3-642-20398-5}
}

@article{RacerD,
  author     = {Blackshear, Sam and Gorogiannis, Nikos and O'Hearn, Peter W. and Sergey, Ilya},
  title      = {RacerD: compositional static race detection},
  year       = {2018},
  issue_date = {November 2018},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {2},
  number     = {OOPSLA},
  url        = {https://doi.org/10.1145/3276514},
  doi        = {10.1145/3276514},
  abstract   = {Automatic static detection of data races is one of the most basic problems in reasoning about concurrency. We present RacerD—a static program analysis for detecting data races in Java programs which is fast, can scale to large code, and has proven effective in an industrial software engineering scenario. To our knowledge, RacerD is the first inter-procedural, compositional data race detector which has been shown to have non-trivial precision and impact. Due to its compositionality, it can analyze code changes quickly, and this allows it to perform continuous reasoning about a large, rapidly changing codebase as part of deployment within a continuous integration ecosystem. In contrast to previous static race detectors, its design favors reporting high-confidence bugs over ensuring their absence. RacerD has been in deployment for over a year at Facebook, where it has flagged over 2500 issues that have been fixed by developers before reaching production. It has been important in enabling the development of new code as well as fixing old code: it helped support conversion of part of the main Facebook Android app from a single-threaded to a multi-threaded architecture. In this paper we describe RacerD’s design, implementation, deployment and impact.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {oct},
  articleno  = {144},
  numpages   = {28},
  keywords   = {Concurrency, Concurrent Separation Logic, Race Freedom, Static Analysis}
}

@inproceedings{ASAN,
  author    = {Konstantin Serebryany and Derek Bruening and Alexander Potapenko and Dmitriy Vyukov},
  title     = {{AddressSanitizer}: A Fast Address Sanity Checker},
  booktitle = {2012 USENIX Annual Technical Conference (USENIX ATC 12)},
  year      = {2012},
  isbn      = {978-931971-93-5},
  address   = {Boston, MA},
  pages     = {309--318},
  url       = {https://www.usenix.org/conference/atc12/technical-sessions/presentation/serebryany},
  publisher = {USENIX Association},
  month     = jun
}

@inproceedings{TSAN,
  author    = {Serebryany, Konstantin and Iskhodzhanov, Timur},
  title     = {ThreadSanitizer: data race detection in practice},
  year      = {2009},
  isbn      = {9781605587936},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1791194.1791203},
  doi       = {10.1145/1791194.1791203},
  abstract  = {Data races are a particularly unpleasant kind of threading bugs. They are hard to find and reproduce -- you may not observe a bug during the entire testing cycle and will only see it in production as rare unexplainable failures. This paper presents ThreadSanitizer -- a dynamic detector of data races. We describe the hybrid algorithm (based on happens-before and locksets) used in the detector. We introduce what we call dynamic annotations -- a sort of race detection API that allows a user to inform the detector about any tricky synchronization in the user program. Various practical aspects of using ThreadSanitizer for testing multithreaded C++ code at Google are also discussed.},
  booktitle = {Proceedings of the Workshop on Binary Instrumentation and Applications},
  pages     = {62-71},
  numpages  = {10},
  keywords  = {testing, dynamic data race detection, concurrency bugs, Valgrind},
  location  = {New York, New York, USA},
  series    = {WBIA '09}
}

@article{SC,
  author   = {Lamport},
  journal  = {IEEE Transactions on Computers},
  title    = {How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs},
  year     = {1979},
  volume   = {C-28},
  number   = {9},
  pages    = {690-691},
  keywords = {Computer design;concurrent computing;hardware correctness;multiprocessing;parallel processing},
  doi      = {10.1109/TC.1979.1675439}
}

@article{TSO,
  author    = {Peter Sewell and
               Susmit Sarkar and
               Scott Owens and
               Francesco Zappa Nardelli and
               Magnus O. Myreen},
  title     = {x86-TSO: a rigorous and usable programmer's model for x86 multiprocessors},
  journal   = {Commun. {ACM}},
  volume    = {53},
  number    = {7},
  pages     = {89--97},
  year      = {2010},
  url       = {https://doi.org/10.1145/1785414.1785443},
  doi       = {10.1145/1785414.1785443},
  timestamp = {Tue, 06 Nov 2018 12:51:37 +0100},
  biburl    = {https://dblp.org/rec/journals/cacm/SewellSONM10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@book{PSO,
  author    = {SPARC International, Inc., CORPORATE},
  title     = {The SPARC architecture manual: version 8},
  year      = {1992},
  isbn      = {0138250014},
  publisher = {Prentice-Hall, Inc.},
  address   = {USA}
}



@article{java,
  author     = {Manson, Jeremy and Pugh, William and Adve, Sarita V.},
  title      = {The Java memory model},
  year       = {2005},
  issue_date = {January 2005},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {40},
  number     = {1},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/1047659.1040336},
  doi        = {10.1145/1047659.1040336},
  abstract   = {This paper describes the new Java memory model, which has been revised as part of Java 5.0. The model specifies the legal behaviors for a multithreaded program; it defines the semantics of multithreaded Java programs and partially determines legal implementations of Java virtual machines and compilers.The new Java model provides a simple interface for correctly synchronized programs -- it guarantees sequential consistency to data-race-free programs. Its novel contribution is requiring that the behavior of incorrectly synchronized programs be bounded by a well defined notion of causality. The causality requirement is strong enough to respect the safety and security properties of Java and weak enough to allow standard compiler and hardware optimizations. To our knowledge, other models are either too weak because they do not provide for sufficient safety/security, or are too strong because they rely on a strong notion of data and control dependences that precludes some standard compiler transformations.Although the majority of what is currently done in compilers is legal, the new model introduces significant differences, and clearly defines the boundaries of legal transformations. For example, the commonly accepted definition for control dependence is incorrect for Java, and transformations based on it may be invalid.In addition to providing the official memory model for Java, we believe the model described here could prove to be a useful basis for other programming languages that currently lack well-defined models, such as C++ and C#.},
  journal    = {SIGPLAN Not.},
  month      = {jan},
  pages      = {378-391},
  numpages   = {14},
  keywords   = {multithreading, memory model, concurrency, Java}
}

@article{c++model,
  author     = {Batty, Mark and Owens, Scott and Sarkar, Susmit and Sewell, Peter and Weber, Tjark},
  title      = {Mathematizing C++ concurrency},
  year       = {2011},
  issue_date = {January 2011},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {46},
  number     = {1},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/1925844.1926394},
  doi        = {10.1145/1925844.1926394},
  abstract   = {Shared-memory concurrency in C and C++ is pervasive in systems programming, but has long been poorly defined. This motivated an ongoing shared effort by the standards committees to specify concurrent behaviour in the next versions of both languages. They aim to provide strong guarantees for race-free programs, together with new (but subtle) relaxed-memory atomic primitives for high-performance concurrent code. However, the current draft standards, while the result of careful deliberation, are not yet clear and rigorous definitions, and harbour substantial problems in their details.In this paper we establish a mathematical (yet readable) semantics for C++ concurrency. We aim to capture the intent of the current (`Final Committee') Draft as closely as possible, but discuss changes that fix many of its problems. We prove that a proposed x86 implementation of the concurrency primitives is correct with respect to the x86-TSO model, and describe our Cppmem tool for exploring the semantics of examples, using code generated from our Isabelle/HOL definitions.Having already motivated changes to the draft standard, this work will aid discussion of any further changes, provide a correctness condition for compilers, and give a much-needed basis for analysis and verification of concurrent C and C++ programs.},
  journal    = {SIGPLAN Not.},
  month      = {jan},
  pages      = {55-66},
  numpages   = {12},
  keywords   = {semantics, relaxed memory models}
}




@article{c++model-proposal,
  author     = {Boehm, Hans-J. and Adve, Sarita V.},
  title      = {Foundations of the C++ concurrency memory model},
  year       = {2008},
  issue_date = {June 2008},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {43},
  number     = {6},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/1379022.1375591},
  doi        = {10.1145/1379022.1375591},
  abstract   = {Currently multi-threaded C or C++ programs combine a single-threaded programming language with a separate threads library. This is not entirely sound [7].We describe an effort, currently nearing completion, to address these issues by explicitly providing semantics for threads in the next revision of the C++ standard. Our approach is similar to that recently followed by Java [25], in that, at least for a well-defined and interesting subset of the language, we give sequentially consistent semantics to programs that do not contain data races. Nonetheless, a number of our decisions are often surprising even to those familiar with the Java effort:We (mostly) insist on sequential consistency for race-free programs, in spite of implementation issues that came to light after the Java work.We give no semantics to programs with data races. There are no benign C++ data races.We use weaker semantics for trylock than existing languages or libraries, allowing us to promise sequential consistency with an intuitive race definition, even for programs with trylock.This paper describes the simple model we would like to be able to provide for C++ threads programmers, and explain how this, together with some practical, but often under-appreciated implementation constraints, drives us towards the above decisions.},
  journal    = {SIGPLAN Not.},
  month      = {jun},
  pages      = {68-78},
  numpages   = {11},
  keywords   = {c++, data race, memory consistency, memory model, sequential consistency, trylock}
}

@article{promise-c++model,
  author     = {Kang, Jeehoon and Hur, Chung-Kil and Lahav, Ori and Vafeiadis, Viktor and Dreyer, Derek},
  title      = {A promising semantics for relaxed-memory concurrency},
  year       = {2017},
  issue_date = {January 2017},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {52},
  number     = {1},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/3093333.3009850},
  doi        = {10.1145/3093333.3009850},
  abstract   = {Despite many years of research, it has proven very difficult to develop a memory model for concurrent programming languages that adequately balances the conflicting desiderata of programmers, compilers, and hardware. In this paper, we propose the first relaxed memory model that (1) accounts for a broad spectrum of features from the C++11 concurrency model, (2) is implementable, in the sense that it provably validates many standard compiler optimizations and reorderings, as well as standard compilation schemes to x86-TSO and Power, (3) justifies simple invariant-based reasoning, thus demonstrating the absence of bad "out-of-thin-air" behaviors, (4) supports "DRF" guarantees, ensuring that programmers who use sufficient synchronization need not understand the full complexities of relaxed-memory semantics, and (5) defines the semantics of racy programs without relying on undefined behaviors, which is a prerequisite for applicability to type-safe languages like Java.  The key novel idea behind our model is the notion of *promises*: a thread may promise to execute a write in the future, thus enabling other threads to read from that write out of order. Crucially, to prevent out-of-thin-air behaviors, a promise step requires a thread-local certification that it will be possible to execute the promised write even in the absence of the promise. To establish confidence in our model, we have formalized most of our key results in Coq.},
  journal    = {SIGPLAN Not.},
  month      = {jan},
  pages      = {175-189},
  numpages   = {15},
  keywords   = {operational semantics, Weak memory models, C++11}
}

@inproceedings{promise2.0-c++model,
  title     = {Promising 2.0: Global optimizations in relaxed memory concurrency},
  abstract  = {For more than fifteen years, researchers have tried to support global optimizations in a usable semantics for a concurrent programming language, yet this task has been proven to be very difficult because of (1) the infamous {"}out of thin air{"} problem, and (2) the subtle interaction between global and thread-local optimizations. In this paper, we present a solution to this problem by redesigning a key component of the promising semantics (PS) of Kang et al. Our updated PS 2.0 model supports all the results known about the original PS model (i.e., thread-local optimizations, hardware mappings, DRF theorems), but additionally enables transformations based on global value-range analysis as well as register promotion (i.e., making accesses to a shared location local if the location is accessed by only one thread). PS 2.0 also resolves a problem with the compilation of relaxed RMWs to ARMv8, which required an unintended extra fence.},
  keywords  = {Compiler Optimizations, Operational Semantics, Relaxed Memory Concurrency},
  author    = {Lee, {Sung Hwan} and Minki Cho and Anton Podkopaev and Soham Chakraborty and Hur, {Chung Kil} and Ori Lahav and Viktor Vafeiadis},
  note      = {Publisher Copyright: {\textcopyright} 2020 ACM.; 41st ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2020 ; Conference date: 15-06-2020 Through 20-06-2020},
  year      = {2020},
  month     = jun,
  day       = {11},
  doi       = {10.1145/3385412.3386010},
  language  = {אנגלית},
  series    = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
  publisher = {Association for Computing Machinery},
  pages     = {362--376},
  editor    = {Donaldson, {Alastair F.} and Emina Torlak},
  booktitle = {PLDI 2020 - Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation}
}

@inproceedings{Owicki-Gries-c11model,
  title     = {Owicki-Gries Reasoning for C11 Programs with Relaxed Dependencies},
  abstract  = {Deductive verification techniques for C11 programs have advanced significantly in recent years with the development of operational semantics and associated logics for increasingly large fragments of C11. However, these semantics and logics have been developed in a restricted setting to avoid the thin-air-read problem. In this paper, we propose an operational semantics that leverages an intra-thread partial order (called semantic dependencies) induced by a recently developed denotational event-structure-based semantics. We prove that our operational semantics is sound and complete with respect to the denotational semantics. We present an associated logic that generalises a recent Owicki-Gries framework for RC11 (repaired C11), and demonstrate the use of this logic over several example proofs.},
  address   = {},
  author    = {Wright, Daniel and Batty, Mark and Dongol, Brijesh},
  editor    = {Huisman, M and Pasareanu, C and Zhan, N},
  doi       = {10.1007/978-3-030-90870-6_13},
  isbn      = {3030908690},
  issn      = {0302-9743},
  journal   = {FORMAL METHODS, FM 2021},
  keywords  = {Computer Science, Interdisciplinary Applications;Computer Science, Theory & Methods;Science & Technology},
  language  = {eng},
  pages     = {18},
  publisher = {Springer Nature},
  series    = {Lecture Notes in Computer Science},
  subject   = {Computer Science;Technology},
  volume    = {13047},
  year      = {2021}
}


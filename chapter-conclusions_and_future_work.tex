\chapter{\label{cha:conclusion}Conclusions and Future
  Work}

This chapter gives an overview of the project's contributions. After
this overview, we will reflect on the results and draw some
conclusions. Finally, some ideas for future work will be discussed.

\section{Contributions}

% \lipsum{3} % add some pseudo content


\section{Conclusions}

% \lipsum{3} % add some pseudo content

\section{Discussion/Reflection}

The structure of our fuzzing framework differs slightly from some existing fuzzers, in the mutation function. For example, AFL uses program inputs as seeds and mutates existing interesting seeds to generate new ones. Similarly, RFF uses abstract schedules as seeds, mutating old schedules to create new ones. The mutation functions in such fuzzers can be expressed as $f_{mutate}: S \to S$, where $S$ represents the set of seeds.  In our framework, however, since seeds are defined as execution graph prefixes, they are mutated directly from execution graphs, which are also treated as the programs' outputs. Hence the mutation function is of the form: $f_{mutate}: G \to S$, where $G$ denotes the set of execution graphs. In some sense, our fuzzer "mutates the program outputs to generate inputs", much resembling a snake biting its own tail. This does not violate the principle of using fuzzing to guide new testing procedures with historical information, which is expected to outperform naive random testing that lacks any feedback and black-box fuzzing, as \cite{sage} suggests. Therefore, one corresponding question to ask is: should we keep track of graphs or prefixes, when they are connected in some way? Our design choice is to keep a list of prefixes, and evaluate their scores according to some metric, just like the way AFL maintains its seeds and performs power scheduling. However, it could be an alternative way to keep track of the execution graphs.

Besides, another design choice we made was about the interesting metic. In our fuzzing algorithm, we consider an execution to be interesting if its graph is new or rare. As a result, the fuzzer will explore as many distinct execution graphs as it can. A question can be asked: why not define interesting graphs as buggy execution graphs (or more interesting, at least), i.e. being biased on those executions with bugs? Since in software testing we often care more about the bugs. One could argue that: 1, if no bugs have been found, the fuzzer still wants to mutate from previous executions; 2, if the tester finds a bug, the user can fix the bug and test it again, without the need of knowing more bugs. On the other hand, one could also suggest: 1, the more, the better; 2, a software testing tool should try harder to detect bugs in principle. In our fuzzer, we took the unbiased approach. Even it does not detect any bug in the end, it can still provide more confidence on the correctness of the user program due to its higher coverage of possible executions.

Lastly, several comments about the thread interleavings. It is known that in weak memory concurrency we use execution graphs, instead of thread interleavings used in SC concurrency, to model executions. Since the SC memory model is a stronger model than weak memory models, the executions allowed under SC should be also allowed by weak memory models, therefore should be captured by execution graph semantics. In other words, execution graphs suffices in a more general sense. However, many model checkers also have a scheduler in their implementations. This scheduler is not the scheduler used for determining thread interleavings under SC, but is used for scheduling the order of adding events to execution graphs. Note that with a prefix $P$ and the next event $e$ in thread $t$, the following two scenarios are equivalent: 1, set $P$ as the prefix of next execution and force the scheduler to pick thread $t$ first, $e$ being the next event to be added; 2, add $e$ to prefix $P$ to compose a new prefix $P' = P \cup \{e\}$ and let the scheduler to add other events randomly. Hence, choosing which one of the two approaches is only an impelmentation issue, not an algorithmic difference. In our implementation on C11Tester, we chose the first approach, but that does not mean the fuzzer is still under the SC semantics.


\section{Future work}

% \lipsum{2} % add some pseudo content

